[ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ–‡æ¡£](#chinese) | [ğŸ‡ºğŸ‡¸ English](#english)


# RL_Learning ğŸ‰ï¸
<a id="chinese"></a>
![Python](https://img.shields.io/badge/Python-3.7%2B-blue) ![çŠ¶æ€](https://img.shields.io/badge/çŠ¶æ€-é‡æ„ä¸­-orange) ![ç®—æ³•](https://img.shields.io/badge/ç®—æ³•-åŸºç¡€RLç®—æ³•-green)


> åŸå§‹ä»£ç æ¥æº: https://github.com/jwk1rose/RL_Learning  
> æœ¬äººæ­£åœ¨é‡æ„ä»£ç ï¼Œå°½é‡åˆ†è§£ä¸ºæ›´å¤šç‹¬ç«‹æ¨¡å—å¹¶æ·»åŠ è¯¦ç»†æ³¨é‡Šã€‚

## ç®€ä»‹ ğŸ“–
æœ¬é¡¹ç›®ä¸ºè¥¿æ¹–å¤§å­¦èµµä¸–é’°è€å¸ˆçš„å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹ä»£ç å®è·µï¼Œç›®å‰å®Œæˆäº†1-9ç« çš„å¤§éƒ¨åˆ†ä»£ç ï¼ŒåŒ…æ‹¬ä»¿çœŸç¯å¢ƒçš„æ­å»ºã€å€¼è¿­ä»£ï¼Œç­–ç•¥è¿­ä»£ã€è’™ç‰¹å¡æ´›ã€æ—¶åºå·®åˆ†ã€çŠ¶æ€å€¼è¿‘ä¼¼ã€DQNã€Reinforce ç­‰ç®—æ³•çš„å®ç°ã€‚å°½å¯èƒ½åœ°è¿½æ±‚å¤ç°ï¼Œä½†æ˜¯ä½œè€…ä»£ç æ°´å¹³æœ‰é™ï¼Œä¸å…å­˜åœ¨è®¸å¤šbugä»¥åŠæ•ˆç‡ä½ä¸‹ä¹‹å¤„ï¼Œè¯·å¤§å®¶ä»…ä½œå‚è€ƒã€‚

éå¸¸å¹¸è¿èƒ½å¤Ÿå‘ç°è¿™ä¸€é—¨è¯¾ï¼Œå› ä¸ºè¿™é—¨è¯¾æˆ‘çŸ¥é“äº†RLã€‚æ¯”è¾ƒè¿‡å¸‚é¢ä¸Šå¾ˆå¤šå…¶ä»–çš„èµ„æ–™ï¼Œä¸ç®¡æ˜¯è¯¾ç¨‹è¿˜æ˜¯æ•™æçš„è´¨é‡éƒ½æ˜¯é¡¶å°–çš„ã€‚åƒèµµè€å¸ˆä¸€æ ·æ„¿æ„è€—è´¹å¦‚æ­¤å¿ƒè¡€ï¼Œåˆ¶ä½œå¦‚æ­¤é«˜è´¨é‡çš„è§†é¢‘çš„è€å¸ˆå·²ç»å¾ˆå°‘äº†ã€‚è°¨ä»¥æ­¤å¼€æºä»“åº“å‘èµµè€å¸ˆè‡´æ•¬âœ‹ã€‚

<div align="center">
  <img src="./scripts/Chapter4_Value iteration and Policy iteration/plot_figure/policy_iteration.png" width="45%" alt="å€¼è¿­ä»£ç®—æ³•å¯è§†åŒ–"/>
  <img src="./scripts/Chapter4_Value iteration and Policy iteration/plot_figure/value_iteration.png" width="45%" alt="ç­–ç•¥æ¢¯åº¦è®­ç»ƒæ›²çº¿"/>
  <p>ä»å·¦åˆ°å³: ç­–ç•¥æ¢¯åº¦ã€å€¼è¿­ä»£ç®—æ³•å¯è§†åŒ–</p>
</div>

## é¡¹ç›®ç»“æ„

```tree
RL_Learning-main/
â”œâ”€â”€ scripts/            # ç®—æ³•å®ç°è„šæœ¬
â”‚   â”œâ”€â”€ Chapter4_Value iteration and Policy iteration/  # ç¬¬4ç« ï¼šå€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£
â”‚   â”œâ”€â”€ Chapter5_Monte Carlo Methods/                  # ç¬¬5ç« ï¼šè’™ç‰¹å¡æ´›æ–¹æ³•
â”‚   â”œâ”€â”€ Chapter6_Stochastic_approximation/            # ç¬¬6ç« ï¼šéšæœºè¿‘ä¼¼
â”‚   â”œâ”€â”€ Chapter7_Temporal-Difference learning/        # ç¬¬7ç« ï¼šæ—¶åºå·®åˆ†å­¦ä¹ 
â”‚   â”œâ”€â”€ Chapter8_Value Function Approximaton/         # ç¬¬8ç« ï¼šå€¼å‡½æ•°è¿‘ä¼¼
â”‚   â”œâ”€â”€ Chapter9_Policy Gradient/                     # ç¬¬9ç« ï¼šç­–ç•¥æ¢¯åº¦
â”‚   â”œâ”€â”€ Chapter10_Actor Critic/                       # ç¬¬10ç« ï¼šæ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•
â”‚   â”œâ”€â”€ grid_env.py                                   # ç½‘æ ¼ç¯å¢ƒ
â”‚   â”œâ”€â”€ model.py                                      # ç¥ç»ç½‘ç»œæ¨¡å‹
â”‚   â”œâ”€â”€ render.py                                     # æ¸²æŸ“å·¥å…·
â”‚   â””â”€â”€ solver.py                                     # æ±‚è§£å™¨åŸºç±»
â””â”€â”€ README.md           # é¡¹ç›®è¯´æ˜
```

## å·²å®ç°ç®—æ³•

| ç®—æ³• | çŠ¶æ€ | ä½ç½® | è¯´æ˜ |
|------|------|------|------|
| å€¼è¿­ä»£ (Value Iteration) | âœ… | `scripts/Chapter4_Value iteration and Policy iteration/` | åŸºäºåŠ¨æ€è§„åˆ’çš„æœ€ä¼˜å€¼å‡½æ•°æ±‚è§£ |
| ç­–ç•¥è¿­ä»£ (Policy Iteration) | âœ… | `scripts/Chapter4_Value iteration and Policy iteration/` | åŸºäºåŠ¨æ€è§„åˆ’çš„æœ€ä¼˜ç­–ç•¥æ±‚è§£ |
| è’™ç‰¹å¡æ´›æ–¹æ³• (Monte Carlo) | âœ… | `scripts/Chapter5_Monte Carlo Methods/` | åŸºäºé‡‡æ ·çš„å€¼å‡½æ•°ä¼°è®¡ |
| æ—¶åºå·®åˆ†å­¦ä¹  (TD Learning) | âœ… | `scripts/Chapter7_Temporal-Difference learning/` | ç»“åˆåŠ¨æ€è§„åˆ’å’Œè’™ç‰¹å¡æ´›çš„æ–¹æ³• |
| Q-learning | âœ… | `scripts/Chapter7_Temporal-Difference learning/` | ç»å…¸çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³• |
| n-step Sarsa | âœ… | `scripts/Chapter7_Temporal-Difference learning/` | å¤šæ­¥æ—¶åºå·®åˆ†å­¦ä¹  |
| çŠ¶æ€å€¼å‡½æ•°è¿‘ä¼¼ (Value Approximation) | âœ… | `scripts/Chapter8_Value Function Approximaton/` | ä½¿ç”¨å‡½æ•°è¿‘ä¼¼ä»£æ›¿è¡¨æ ¼å‹è¡¨ç¤º |
| DQN (Deep Q-Network) | âœ… | `scripts/Chapter8_Value Function Approximaton/` | æ·±åº¦Qç½‘ç»œç®—æ³• |
| Reinforce ç®—æ³• | âœ… | `scripts/Chapter9_Policy Gradient/` | åŸºç¡€ç­–ç•¥æ¢¯åº¦ç®—æ³• |
| Actor-Critic | âœ… | `scripts/Chapter10_Actor Critic/` | ç»“åˆç­–ç•¥æ¢¯åº¦å’Œå€¼å‡½æ•°è¿‘ä¼¼çš„æ–¹æ³• |


## å¼€å‘ç¯å¢ƒè¯´æ˜
### PyCharmç”¨æˆ·æ³¨æ„äº‹é¡¹
ä½¿ç”¨**PyCharm**æ‰“å¼€æœ¬é¡¹ç›®æ—¶ï¼Œä»£ç ä¸­çš„ `sys.path.append("..")` å¯¼å…¥è¯­å¥ä¸ä¼šæŠ¥é”™ã€‚**PyCharm**ä¼šè‡ªåŠ¨å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°`PYTHONPATH`ä¸­ï¼Œç¡®ä¿æ¨¡å—å¯¼å…¥æ­£å¸¸å·¥ä½œã€‚

å¦‚æœæ‚¨ä½¿ç”¨å…¶ä»–IDEæˆ–ç›´æ¥é€šè¿‡å‘½ä»¤è¡Œè¿è¡Œè„šæœ¬ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨è®¾ç½®`PYTHONPATH`ç¯å¢ƒå˜é‡
```bash
export PYTHONPATH=$PYTHONPATH:/path/to/RL_Learning-main
```
æˆ–
```python
# sys.path.append("..") # æ³¨é‡Šæ‰æ­¤è¡Œ
# æ”¹ä¸ºï¼š
import os 
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
```

## æ›´æ–°æ—¥å¿—

<a id="fix-issue-7"></a>
**2026.2.19**  
1. ä¿®å¤ [issue-7](https://github.com/Ronchy2000/Multi-agent-RL/issues/7)ï¼šä¿®æ­£ `scripts/Chapter5_Monte Carlo Methods/MC_Basic.py` ä¸­ `mc_basic_simple` ä¸ `mc_basic_simple_GUI` çš„ç¼©è¿›é”™è¯¯ã€‚  
  - `sum_qvalue_list.append(sum_qvalue)` ç°åœ¨ä½äº `for each_episode in episodes:` å¾ªç¯å†…éƒ¨ï¼Œç¡®ä¿æ¯æ¡ episode çš„å›æŠ¥éƒ½è¢«ç»Ÿè®¡ã€‚  
  - ä¿®å¤å `self.qvalue[state][action] = np.mean(sum_qvalue_list)` å°†åŸºäºå®Œæ•´é‡‡æ ·é›†åˆè®¡ç®—å‡å€¼ï¼Œä¸å†åªä½¿ç”¨æœ€åä¸€æ¡ episode çš„å›æŠ¥ã€‚  

<a id="fix-issue-1"></a>
**2026.2.15**  
1. ä¿®å¤ [issue-1](https://github.com/Ronchy2000/Multi-agent-RL/issues/1)ï¼šç¬¬8ç«  TD-Linearï¼ˆçº¿æ€§å‡½æ•°é€¼è¿‘ï¼‰å®ç°ä¸­çš„ä¸¤ä¸ªé—®é¢˜ï¼š  
  - ä¿®æ­£ `scripts/Chapter8_Value Function Approximaton/1.TD-Linear.py` ä¸­ `reward_list` ä¸ `scripts/grid_env.py` çš„ `Rsa` å¥–åŠ±ç´¢å¼•é¡ºåºä¸ä¸€è‡´çš„é—®é¢˜ï¼ˆç´¢å¼•çº¦å®šå›ºå®šä¸º `[other, target, forbidden, overflow]`ï¼‰ï¼Œé¿å… `policy_evaluation()` å¾—åˆ°é”™è¯¯çš„çŠ¶æ€å€¼ã€‚  
  - `scripts/grid_env.py` æ–°å¢ `reward_list` å¯é€‰å‚æ•°ï¼Œä½¿æ¯ä¸ªç®—æ³•è„šæœ¬éƒ½å¯ä»¥é€šè¿‡ `GridEnv(..., reward_list=[...])` ç‹¬ç«‹é…ç½®å¥–åŠ±å‡½æ•°ï¼ˆæ— éœ€æ‰‹åŠ¨æ”¹ `grid_env.py`ï¼‰ã€‚  
  - ä¿®æ­£ TD(0) çº¿æ€§å‡½æ•°é€¼è¿‘æƒé‡æ›´æ–°é—æ¼ `phi(s_t)` çš„é—®é¢˜ï¼š`w <- w + alpha * delta_t * phi(s_t)`ï¼Œå…¶ä¸­ `delta_t = r + gamma * phi(s_{t+1})^T w - phi(s_t)^T w`ã€‚  


**2024.6.7**  
é‡å¤§æ›´æ–°ï¼åŸä½œè€…çš„æ¸²æŸ“åæ ‡ä¸çŠ¶æ€è®¾ç½®ä¸ä¸€è‡´ï¼Œç°å·²ç»Ÿä¸€åæ ‡ä¸ºï¼š  
![img.png](../img.png)

## ç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n rl_learning python=3.7
conda activate rl_learning

# å®‰è£…ä¾èµ–
pip install numpy matplotlib torch gymnasium tensorboard
```

## ä½¿ç”¨ç¤ºä¾‹
```bash
# è¿è¡Œå€¼è¿­ä»£ç®—æ³•
python scripts/chapter4/value_iteration.py

# è¿è¡ŒDQNç®—æ³•
python scripts/chapter8/dqn.py
```

## å‚è€ƒèµ„æ–™

- [èµµä¸–é’°è€å¸ˆè¯¾ç¨‹åœ°å€](https://www.bilibili.com/video/BV1sd4y167NS) ğŸ’Œ
- [å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸºç¡€](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning)

## è´¡çŒ®æŒ‡å—

æ¬¢è¿å¯¹æœ¬é¡¹ç›®è¿›è¡Œè´¡çŒ®ï¼æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å‚ä¸ï¼š
1. æäº¤IssueæŠ¥å‘Šbugæˆ–æå‡ºæ”¹è¿›å»ºè®®
2. æäº¤Pull Requestä¿®å¤bugæˆ–æ·»åŠ æ–°åŠŸèƒ½
3. å®Œå–„æ–‡æ¡£å’Œæ³¨é‡Š

## è‡´è°¢

æ„Ÿè°¢è¥¿æ¹–å¤§å­¦èµµä¸–é’°è€å¸ˆçš„ç²¾å½©è¯¾ç¨‹å’ŒåŸä½œè€…jwk1roseçš„å¼€æºè´¡çŒ®ã€‚

---

[ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ–‡æ¡£](#chinese) | [ğŸ‡ºğŸ‡¸ English](#english)

<a id="english"></a>
# RL_Learning ğŸ‰ï¸

![Python](https://img.shields.io/badge/Python-3.7%2B-blue) ![Status](https://img.shields.io/badge/status-refactoring-orange) ![Algorithms](https://img.shields.io/badge/algorithms-basic%20RL-green)

> Original code source: https://github.com/jwk1rose/RL_Learning  
> I am refactoring the code, trying to divide it into more independent modules and adding detailed comments.

## Introduction ğŸ“–
This project implements the reinforcement learning course code from Professor Shiyu Zhao at Westlake University. It covers most of the code from chapters 1-9, including the construction of simulation environments, value iteration, policy iteration, Monte Carlo methods, temporal difference learning, state value approximation, DQN, Reinforce, and other algorithms. While striving for accurate reproduction, the author's coding skills are limited, so there may be bugs and inefficiencies. Please use it only as a reference.

I was very fortunate to discover this course, as it introduced me to reinforcement learning. Compared to many other resources available, both the course and textbook quality are top-notch. Professors like Prof. Zhao who are willing to invest so much effort in creating such high-quality videos are rare nowadays. I dedicate this open-source repository to honor Professor Zhaoâœ‹.

<div align="center">
  <img src="./scripts/Chapter4_Value iteration and Policy iteration/plot_figure/policy_iteration.png" width="45%" alt="Policy Iteration Visualization"/>
  <img src="./scripts/Chapter4_Value iteration and Policy iteration/plot_figure/value_iteration.png" width="45%" alt="Value Iteration Visualization"/>
  <p>From left to right: Policy Iteration, Value Iteration Visualization</p>
</div>

## Project Structure

```tree
RL_Learning-main/
â”œâ”€â”€ scripts/            # Algorithm implementation scripts
â”‚   â”œâ”€â”€ Chapter4_Value iteration and Policy iteration/  # Chapter 4: Value Iteration and Policy Iteration
â”‚   â”œâ”€â”€ Chapter5_Monte Carlo Methods/                  # Chapter 5: Monte Carlo Methods
â”‚   â”œâ”€â”€ Chapter6_Stochastic_approximation/            # Chapter 6: Stochastic Approximation
â”‚   â”œâ”€â”€ Chapter7_Temporal-Difference learning/        # Chapter 7: Temporal Difference Learning
â”‚   â”œâ”€â”€ Chapter8_Value Function Approximaton/         # Chapter 8: Value Function Approximation
â”‚   â”œâ”€â”€ Chapter9_Policy Gradient/                     # Chapter 9: Policy Gradient
â”‚   â”œâ”€â”€ Chapter10_Actor Critic/                       # Chapter 10: Actor-Critic Methods
â”‚   â”œâ”€â”€ grid_env.py                                   # Grid environment
â”‚   â”œâ”€â”€ model.py                                      # Neural network models
â”‚   â”œâ”€â”€ render.py                                     # Rendering tools
â”‚   â””â”€â”€ solver.py                                     # Base solver class
â””â”€â”€ README.md           # Project description
```

## Implemented Algorithms

| Algorithm | Status | Location | Description |
|------|------|------|------|
| Value Iteration | âœ… | `scripts/Chapter4_Value iteration and Policy iteration/` | Optimal value function solving based on dynamic programming |
| Policy Iteration | âœ… | `scripts/Chapter4_Value iteration and Policy iteration/` | Optimal policy solving based on dynamic programming |
| Monte Carlo Methods | âœ… | `scripts/Chapter5_Monte Carlo Methods/` | Value function estimation based on sampling |
| TD Learning | âœ… | `scripts/Chapter7_Temporal-Difference learning/` | Methods combining dynamic programming and Monte Carlo |
| Q-learning | âœ… | `scripts/Chapter7_Temporal-Difference learning/` | Classic off-policy reinforcement learning algorithm |
| n-step Sarsa | âœ… | `scripts/Chapter7_Temporal-Difference learning/` | Multi-step temporal difference learning |
| Value Approximation | âœ… | `scripts/Chapter8_Value Function Approximaton/` | Using function approximation instead of tabular representation |
| DQN (Deep Q-Network) | âœ… | `scripts/Chapter8_Value Function Approximaton/` | Deep Q-Network algorithm |
| Reinforce Algorithm | âœ… | `scripts/Chapter9_Policy Gradient/` | Basic policy gradient algorithm |
| Actor-Critic | âœ… | `scripts/Chapter10_Actor Critic/` | Methods combining policy gradient and value function approximation |

## Development Environment Notes
### Note for PyCharm Users
When opening this project with **PyCharm**, the `sys.path.append("..")` import statements in the code will work without errors. **PyCharm** automatically adds the project root directory to `PYTHONPATH`, ensuring that module imports work correctly.

If you are using another IDE or running scripts directly from the command line, you may need to manually set the `PYTHONPATH` environment variable:

```bash
export PYTHONPATH=$PYTHONPATH:/path/to/RL_Learning-main
```
or
```python
# sys.path.append("..") # Comment out this line
# Change to:
import os 
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
```

## Update Log

<a id="fix-issue-7-en"></a>
**2026.2.19**  
Fix [issue-7](https://github.com/Ronchy2000/Multi-agent-RL/issues/7): fixed an indentation bug in `scripts/Chapter5_Monte Carlo Methods/MC_Basic.py` (`mc_basic_simple` and `mc_basic_simple_GUI`).  
- `sum_qvalue_list.append(sum_qvalue)` is now inside the `for each_episode in episodes:` loop, so every sampled episode return is included.  
- After the fix, `self.qvalue[state][action] = np.mean(sum_qvalue_list)` uses the full sampled set instead of only the last episode return.  

<a id="fix-issue-1-en"></a>
**2026.2.15**  
Fixes for Chapter 8 TD-Linear (linear function approximation):  
- Align `reward_list` in `scripts/Chapter8_Value Function Approximaton/1.TD-Linear.py` with `scripts/grid_env.py`'s `Rsa` reward-index convention by using `env.reward_list` (`[other, target, forbidden, overflow]`), so `policy_evaluation()` computes correct state values.  
- Make `scripts/grid_env.py` accept an optional `reward_list` argument so each algorithm script can configure its own reward scheme via `GridEnv(..., reward_list=[...])`.  
- Fix missing `phi(s_t)` in the TD(0) weight update: `w <- w + alpha * delta_t * phi(s_t)`.  
**2024.6.7**  
Major update! The original author's render coordinates were inconsistent with the state settings. The coordinates have been unified as:  
![img.png](../img.png)

## Environment Setup

```bash
# Create virtual environment
conda create -n rl_learning python=3.7
conda activate rl_learning
# Install dependencies
pip install numpy matplotlib torch gymnasium tensorboard

```


## Usage Examples
```bash
# Run value iteration algorithm
python scripts/Chapter4_Value\ iteration\ and\ Policy\ iteration/value_iteration.py

# Run Q-learning algorithm
python scripts/Chapter7_Temporal-Difference\ learning/3.Q-learning.py

# Run DQN algorithm
python scripts/Chapter8_Value\ Function\ Approximaton/DQN.py
```


## References

- [Professor Zhao's Course](https://www.bilibili.com/video/BV1sd4y167NS) ğŸ’Œ
- [Mathematical Foundation of Reinforcement Learning](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning)

## Contribution Guidelines

Contributions to this project are welcome! You can participate in the following ways:
1. Submit issues to report bugs or suggest improvements
2. Submit pull requests to fix bugs or add new features
3. Improve documentation and comments

## Acknowledgements

Thanks to `Professor Shiyu Zhao` from Westlake University for his excellent course and the original author `jwk1rose` for the open-source contribution.
